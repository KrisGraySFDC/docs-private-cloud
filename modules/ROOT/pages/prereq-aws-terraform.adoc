= Create Anypoint Private Cloud Resources on Amazon Web Services (AWS)
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]

This topic describes how to create the resources required to install Anypoint Platform Private Cloud Edition on Amazon Web Services (AWS). Anypoint Private Cloud supports 3-node and 6-node configurations in a production environment on AWS.

== Prerequisites

To install Anypoint Platform Private Cloud Edition on AWS, you must have an AWS account with the following:

* Your account must have AWS keys with EC2FullAccess permissions.
* When creating your AWS environment, the following resources are created:
+
[%header%autowidth.spread]
|===
| AWS Resource | Number required (3-node) | Number required (6-node)
| m4.2xlarge | 3 | 6
| root disk @ 500 iops | 3 | 6
| EBS volumes @ 1500 iops | 6 | 12
| EBS volume @ 3000 iops | 6 | 12
| Amazon ELB | 1 | 1
| t2.medium | 1 | 1
|===

== Run the AWS Provisioner

MuleSoft provides a Docker image that you can use to provision the resources for your AWS account.

. Create an initial instance (t2.small) on your AWS account on any VPC with internet access.
+
This instance is used to run the actual provisioning of the cluster. You must use an AMI that has Docker installed by default or you must manually install Docker after creating the AWS instance.
+
You can also run provisioner remotely from any other machine with Docker and internet access.

. Download the provisioner Docker image from the following URL:
+
----
https://onprem-standalone-installers.s3.amazonaws.com/pce2.0-aws-provisioner.gz?Signature=hSgd6Vap4277eyibvfBqcXAJbqk%3D&Expires=1550697034&AWSAccessKeyId=AKIAI3RYALJL3PWNBRSQ
----

Copy the provisioner Docker image to the instance via SCP

----
scp -i <guest>.pem ~/Downloads/pce2.0-aws-provisioner.gz ec2-user@18.221.155.87:/home/ec2-user
----

. SSH into the instance
+
----
ssh -i 'anypoint.pem' ec2-user@18.221.155.87
----

. Create the var file (pce.env) with the environment details using the template below:

[%header%autowidth.spread]
|===
| AWS_ACCESS_KEY_ID | Specifies the AWS access ID Terraform uses to connect to your AWS account.
| AWS_SECRET_ACCESS_KEY | Specifies the AWS access key Terraform uses to connect to your AWS account.
| AWS_KEY_NAME | Specifies the AWS SSH key. Do not include the .pem extension.
| AWS_REGION | Specifies the AWS region where Terraform creates the cluster. For example: `us-east-2`
|===

You can also include the following set of optional environment variables:

[%header%autowidth.spread]
|===
| TF_VAR_ssh_user=<SSH_USER> | Specifies the ssh user. For example: `ec2-user`, `centos`, etc)
| TF_VAR_monitoring=<true or false> | If true it will create basic instance alarms in Cloudwatch.
| TF_VAR_use_bastion=<true or false> | If true it will create a small instance (using an ASG) in a public subnet as a jumpbox and associate a public IP address, and launch the cluster instances in the private subnets.
| TF_VAR_internal=<true or false> | If true the cluster instances will be launched in private subnets and will not have public ips associated with them. The provisioned load balancer will be internal only.
| AWS_VPC_ID=<VPC_ID> | Specifies an existing VPC. It should already have an internet gateway attached to it. Subnets will still be provisioned within the existing VPC.
| TF_VAR_vpc_cidr=<VPC_CIDR> | You can pass a preferred CIDR for the new VPC that will be provisioned.
| TF_VAR_subnets=<PRIVATE_SUBNETS_CIDRs> | You can pass the preferred CIDRs blocks for the new subnets that will be provisioned. You must pass exactly the minimum num	ber between the number of AZs of the region and the number of nodes you selected for your cluster
| TF_VAR_public_subnets=<PUBLIC_SUBNETS_CIDRs> | You can pass the preferred CIDRs blocks for the new subnets that will be provisioned. You must pass exactly min(length(var.azs), var.node_count)
| TF_VAR_role_tag_value=<AWS_TAG_VALUE_FOR_ROLE_LABEL> | You can provide a value for a ROLE tag that all AWS resources will have
|===

. Load the provisioner Docker image into the local Docker registry:
+
----
docker load -i pce2.0-aws-provisioner.gz
----

. Record the image ID after running the `docker load` command. This is required in the following step.

.  Run the provisioner
+
----
docker run --rm --env-file pce.env IMAGE_ID cluster-provision
----
+
After the provisioner runs successfully, it displays information about your environment including IP addresses, DNS name of the load balancer, etc.

. Verify that the provisioning script ran successfully by checking the existence of /var/lib/bootstrap_complete on the instances

== Open Port 61009 Before Installation (Optional)

If you are installing Anypoint Private Cloud Edition using the GUI-based installer, you must enable this port before running the installer. You must open this port to the world in the cluster's security group before running the installer using AWS Web Console.

== Install Anypoint Platform Private Cloud Edition

After provisioning resources in your AWS environment and uploading the installer to one of the nodes, install Anypoint Platform Private Cloud Edition using one of the installers:

* xref:install-installer.adoc[To Install Anypoint Private Cloud using the GUI Installer]
* xref:install-auto-install.adoc[To Install Anypoint Private Cloud using the Command Line Installer]

== Disabling Port 61009 After Installation

After installation is finished you can close port 61009 in the cluster security group using the AWS Web console.

== See Also

* https://www.terraform.io/intro/getting-started/install.html[Install Terraform]
* https://docs.aws.amazon.com/quickstart/latest/linux-bastion/welcome.html[Linux Bastion Hosts on the AWS Cloud: Quick Start Reference Deployment]